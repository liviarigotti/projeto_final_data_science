# -*- coding: utf-8 -*-
"""projeto_data_science_livia_rigotti.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RGuWj1D7Qc8mN8MUNI22LkroD8W46YxA
"""

#Começo importando as bilbiotecas antes de importar o dataset

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

#Aqui eu dei acesso ao conteúdo do meu drive

from google.colab import drive
drive.mount('/content/drive')

#Lendo o arquivo através do caminho do drive

df1=pd.read_csv ("/content/drive/MyDrive/Projeto Data Science/US Superstore data.xls - Orders.csv")

#Verificando as 5 primeiras linhas para entender o dataset

df1.head(5)

#Verificando se há valores nulos em todas as colunas

print(df1.isnull().sum())

#Quando rodei o df1.head(5), pensei em fazer a análise a partir das datas e categorias em relação as vendas e lucro. Por isso limpei as colunas abaixo

df1 = df1.drop(['Order ID', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Postal Code', 'Region', 'Product ID', 'Sub-Category', 'Product Name', 'Row ID'], axis=1)

#Verificando os dados após limpeza das colunas

df1.head(5)

#Para seguir com as análises, verifiquei antes os types

print(df1.info())

#Ajustei Sales, Profit e Discount para float para seguir com a anális exploratória

df1['Sales'] = df1['Sales'].str.replace(',', '')
df1['Sales'] = df1['Sales'].astype(float)

df1['Profit'] = df1['Profit'].str.replace(',', '')
df1['Profit'] = df1['Profit'].astype(float)

df1['Discount'] = df1['Discount'].str.replace(',', '')
df1['Discount'] = df1['Discount'].astype(float)

#Análise exploratória para retirar insights das métricas

print(df1.describe())

#Aqui eu passei a coluna Order Date para o formato datime para conseguir trabalhar com ela como uma data

df1['Order Date'] = pd.to_datetime(df1['Order Date'])

min_date = df1['Order Date'].min()
max_date = df1['Order Date'].max()

print(f"A data mínima é {min_date} e da data máxima é {max_date}")

#Criei uma nova coluna 'year' com os anos extraídos da coluna Order Date para fazer a análise por ano já que os dados contém 4 anos e inúmeras datas
df1['year'] = df1['Order Date'].dt.year

#Aqui a ideia foi contar a frequência de cada ano para identificar qual ano teve maior volume de vendas
year_counts = df1['year'].value_counts()

#Eu poderia printar o 'year_counts' para identificar a olho nu o ano com maior frequência, porém queria construir uma frase afirmando isso conforme print abaixo
most_common_year = year_counts.idxmax()

print(f"O ano mais frequente no dataset é {most_common_year}.")

"""Abaixo quis identificar a categoria de produto mais vendida."""

#Ainda analisando todos os dados, qual foi a categoria mais vendida ao longo destes anos?
categorias_mais_vendidas = df1.groupby('Category')['Quantity'].sum()
print(categorias_mais_vendidas)

#Conclusão a partir dos dados da 'categoria_mais_vendidas'
print("A categoria mais vendida, considerando todos os anos, é a Office Supplies")

#Agrupei os dados por ano para calcular a soma das vendas para cada ano
year_sales_sum = df1.groupby('year')['Sales'].sum()
print(year_sales_sum)

#Reforçando o ano com maior número de vendas, porém agora verificando isso ano a ano
best_selling_year = year_sales_sum.idxmax()


#O valor das vendas(faturamento) do ano com o maior volume de vendas
total_sale = year_sales_sum.max()
print(total_sale)

print(f'O ano de {best_selling_year} foi o que teve maior número de vendas e a soma dessas vendas totalizam o valor de ${total_sale}')

#Copiei os valores adquiridos a partir da 'year_sales_sum', coloquei em series para fazer um gráfico do tipo histograma
data = {'Year': [2014, 2015, 2016, 2017],
        'Sales': [216122757.0, 250505855.0, 309270698.0, 355692410.0],
        }
df2 = pd.DataFrame(data)

#Histograma
plt.figure(figsize=(3, 5))
plt.bar(df2['Year'], df2['Sales'])
plt.xlabel('Ano')
plt.ylabel('Vendas')
plt.title('Faturamento por Ano')
plt.xticks(df2['Year'])
plt.grid(True)
plt.show()

#Aqui eu uni ano e quantidade para identificar o volume vendido por ano
quantity_per_year = df1.groupby('year')['Quantity'].sum()
print(quantity_per_year)

#Series dos quantity_per_year
quantity_per_year = pd.Series(data=[7581, 7979, 9837, 12476], index=[2014, 2015, 2016, 2017], name='Quantity')
#Series em DataFrame
df_quantity_per_year = quantity_per_year.reset_index()

#Histograma
plt.figure(figsize=(3, 5))
sns.set(style="whitegrid")
sns.barplot(x='index', y='Quantity', data=df_quantity_per_year)
plt.title('Volume por ano')
plt.xlabel('Ano')
plt.ylabel('Quantidade')
plt.show()

"""A diferença entre os histogramas acima é que um mostra o Faturamento(Sales) por ano e o outro mostra o Volume(Quantity) de vendas por ano. Na apresentação detalhei isso melhor.

---

A partir de agora eu comecei a focar no ano com mais Faturamento e mais Volume e relacionei esse ano com as categorias(Category). ⬇
"""

#Filtrando os dados criando um novo df só para o ano de 2017
df2017 = df1[df1['year'] == 2017]

#Uni a categoria ao volume para entender a representatividade(%) de cada categoria através do gráfico de setores
quantity_per_category = df2017.groupby('Category')['Quantity'].sum().reset_index()
print(quantity_per_category)

#Gráfico de setores:
labels = ['Furniture', 'Office Supplies','Technology']
vals = [2437, 7676, 2363]

fig, ax = plt.subplots(figsize=(12, 5))
ax.pie(vals, labels=labels, autopct="%.1f%%")
ax.set_title('Representatividade do volume das categorias vendidas em 2017', fontsize = 10)
plt.show()

#Valor de vendas por categoria unindo as colunas Category e Sales para identificar através de um histograma o faturamento por categoria
sales_per_category = df2017.groupby('Category')['Sales'].sum().reset_index()
print(sales_per_category)

#Gráfico de barras:
plt.figure(figsize=(8,6))
sns.set(style="whitegrid")
sns.barplot(x='Category', y='Sales', data=sales_per_category)
plt.title('Faturamento por categoria em 2017')
plt.xlabel('Categoria')
plt.ylabel('Valor vendido')
plt.show()

# Criação do DataFrame sem atribuí-lo a uma lista
profit_per_category = df2017.groupby('Category')['Profit'].sum().reset_index()

# Agora, ao imprimir profit_per_category, ele será exibido como um DataFrame
print(profit_per_category)

#Unindo Categoria(Category) com Lucro(Profit) para identificar a categoria de maior lucro, depois de identificar a de maior faturamento e maior volume.
profit_per_category = df2017.groupby('Category')['Profit'].sum().reset_index()
print(profit_per_category)

#Criando um histograma com os dados da 'profit_per_category'
plt.figure(figsize=(8, 6))
sns.barplot(x='Profit', y='Category', data=profit_per_category)
plt.title('Lucro por categoria em 2017')
plt.xlabel('Lucro')
plt.ylabel('Categoria')
plt.show()

"""# ** MODELO MACHINE LEARNING**:

---

A partir de agora irei para a etapa do modelo de Machine Learning

"""

#Criando o mapa de calor para identificar as correlações como primeiro passo
correlacoes = df1.corr()
print(correlacoes)

plt.figure(figsize=(12,6))
sns.set_theme(style="white")
corr = df1.corr()
heatmap = sns.heatmap(corr, annot=True, cmap = 'GnBu')

#Importando de treinamento e teste a de regressão linear para prever dados a partir da maior correlação: Discount X Sales
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

#Dividindo os dados em conjuntos de treinamento e teste
X = df1['Sales']
y = df1['Discount']

#Extraindo Sales e Discount e redimensionando para uma matriz 2D
X = df1['Sales'].values.reshape(-1, 1)
y = df1['Discount'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Criando e treinando o modelo de regressão linear
model = LinearRegression()
model.fit(X_train, y_train)

#Identificar os valores do Coeficiente angular e Intercepto para ajudar na análise
print('Coeficiente angular (slope):', model.coef_[0])
print('Intercepto:', model.intercept_)
print('Fórmula da Regressão: y = X*', model.coef_[0], '+', model.intercept_)

#Previsões
y_pred = model.predict(X_test)
print(y_pred)

#Avaliação do modelo de regressão linear
from sklearn.metrics import mean_squared_error, r2_score
print('Erro quadrático médio:', mean_squared_error(y_test, y_pred))
print('Coeficiente de determinação (R²):', r2_score(y_test, y_pred))

from sklearn import metrics
#Verificando a precisando através do Erro Médio Absoluto

#Validando do modelo pelo calculo do erro absoluto
prediction = model.predict(X_train)
print("MAE Resultado = " , metrics.mean_absolute_error(y_train, prediction))

prediction2 = model.predict(X_test)
print("MAE Resultado = " , metrics.mean_absolute_error(y_test, prediction2))

#Esse gráfico foi criado para me ajudar a visualizar e interpretar de maneira geral todos os dados gerados para regressão linear, incluindo com o Intercepto


sales = df1['Sales']
discount = df1['Discount']
slope = 2.3060752629532272e-06
intercept = 1.5109477440862675

#Gráfico de dispersão
plt.figure(figsize=(8, 6))
plt.scatter(sales, discount, alpha=0.5, label='Dados')

#Linha de regressão
x_values = np.linspace(min(sales), max(sales), 100)
y_values = slope * x_values + intercept
plt.plot(x_values, y_values, color='red', label='Linha de Regressão')

#Intercepto
plt.axhline(y=intercept, color='green', linestyle='--', label='Intercepto')


plt.title('Gráfico de Dispersão com Linha de Regressão e Intercepto')
plt.xlabel('Sales')
plt.ylabel('Discount')
plt.legend()
plt.grid(True)

# Exibir o gráfico
plt.show()